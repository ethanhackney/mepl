slab_base
=========
struct ptr *(*sb_get)(struct pool *pool)
void       (*sb_put)(struct pool *pool, struct ptr *ptr)

slab<T>
=======
struct slab_base s_base
struct ring<T>   s_ring
struct list      s_nodes
size_t           s_pool_sz
size_t           s_ring_sz

slab_node<T>
============
struct bitset<T> sn_set
struct list      sn_nodes
size_t           sn_count
T                *sn_pool

the slab tab can be a perfect hash table because there are a fixed number
of slabs (one for each type)

O(1) to find a slab{}
O(1) to allocate from a slab{}
O(1) to reach slab_node{} from ptr{}
O(1) to remove slab_node{} from list{}

hash_map<str,slab_base*> slab_tab {
        { "AST_STR", &ast_str_slab.ss_slab },
        { "AST_VAR_DEF", &ast_var_def_slab.vds_slab },
};

slab_node{
        .arr   = [ ][ ][ ][ ][ ]    # fixed size array
        .set   = 000000000000000000 # bitset of (len(.arr) >> 64) words
        .count = 5                  # count of available nodes
        .nodes = { }<-+             # circular doubly linked list of all slab nodes
}                 ^   |             # the same list node is used for slab.avail
                  +---+             # and slab.empty because they are mutually
                                    # exclusive

slab{
                  
        .avail = { }<-- # circular doubly linked list of all non-empty slab nodes
                  ^   |
                  +---+
        .empty = { }<-+ # circular doubly linked list of all empty slab nodes
                  ^   |
                  +---+
        .ring  = [ ][ ][ ][ ] # fixed size ring buffer of pointers to unused nodes
                  |  |  |  |  # hot nodes come off first, when it gets full then
                  |  |  |  |  # free() is called on the coldest node
                  |  |  |  |
                  |  |  |  +--------> { free slab node kept for future use }
                  |  |  +-----------> { free slab node kept for future use }
                  |  +--------------> { free slab node kept for future use }
                  +-----------------> { free slab node kept for future use }
}

slab_tab{
        # hash table mapping type names to slabs that provide that type
}

the hash table based thing is dumb, it doesnt actually help with decoupling
and only slows things down

slabhash = {
        ["AST_VAR_DEF"] ==> {                 |  |  |  |
                                .s_ring[]  = [ ][ ][ ][ ]
                                .s_nodes[] = { circular list of bitset + array },
                            }
}
                                     +------------------> { free cached slab_node }
                                     |  +---------------> { free cached slab_node }
                                     |  |  +------------> { free cached slab_node }
["AST_VAR_DEF"]=====>{               |  |  |
                        .s_ring[] = [ ][ ][ ]

                                     +------{ slab_node }-------{ slab_node }--------+
                                     |                                               
                        .s_nodes  = { }                                              |
                                     |                                               |
                                     +-----------------------------------------------+

REMEMBER: bitset keeps index of next entry with free bits so finding next
word with free bits is O(1) and __builtin_ctzll() is also O(1)

time complexity break down
==========================

slab_init():
------------
O(1)

slab_get():
-----------
O(1)                 if avail list is non-empty
O(1)                 if avail list is empty, but ring is non-empty
O(malloc(slab_node)) if avail list is empty, and ring is empty (basically never happens)
O(slab_node_get())
O(1)                 if node is empty, move to empty list (list removal/insertion)

slab_node_get():
----------------
O(1)                           for finding a free entry (bitset next free)
O(malloc(sizeof(struct ptr))) (probably O(1) because 'struct ptr's are only 16 bytes on 64-bit)

slab_put():
----------
O(slab_node_put())
O(free(slab_node)) if ring is full (probably O(1) for same reasons as free(ptr))
O(1)               for ring management (pop() + push() if full, or just push() if not)
O(1)               for moving node to head of avail list if count > head(avail)->count

slab_node_put():
----------------
O(1)          for freeing entry
O(free(ptr)) (probably O(1) because all free() will have to do is coalesce in the worst case)
